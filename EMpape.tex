\documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array}
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{subfigure} %allows to figures on the same line
\usepackage{bbm} % for indicator functions, lol
\usepackage[margin=1in]{geometry} %change the margins, lol
\usepackage{cite} %CITING STUFF!!!
\setlength{\parskip}{1ex} %--skip lines between paragraphs
%\setlength{\parindent}{0pt} %--don't indent paragraphs
	\addtolength{\oddsidemargin}{-.25in} %move the margins slightly to the left
\linespread{1.5}
\usepackage{titling}

\setlength{\droptitle}{-5em} 
%-- Commands for header


%\linespread{2} %-- Uncomment for Double Space
\begin{document}
\title{EM over areas of a network}
\author{David Sinclair\\
dgs242}
\date{\today}
\maketitle

\section{Framework}
Take a graph $G = (V, E)$.  Let $Z^n \in \{0, 1\}^{|V|}$ be the true node values from an Ising distribution, such that $Z^n \stackrel{iid}{\sim} Ising(\theta, G)$.  Let $\hat{Z}^n$ be the observed node values, following a misclassified Ising distribution, i.e. $\hat{Z}^n \stackrel{iid}{\sim} Ising_m(\theta, G, \tilde\gamma)$, which is defined by $P(\hat{Z}^n_i \neq Z^n_i ) = \tilde\gamma_i$ independently for all $i$.  Let $\hat{Z}$ be all misclassified observations.

Now, let $M_g = \{i : \tilde\gamma_i > g\}$.  These will be the misclassified nodes whose neighbors we will re-estimate.  Our final goal will be to choose $g$ such that we can produce the best estimate of $E$.  For the purposes of the examples in this document, $g$ will be known, due to $\tilde\gamma$ having only a few large values.  In practice our choice of $g$ will almost certainly be that which allows for a computationally tractable method, since the EM algorithm will become computationally intractable over only a small number of nodes.

\section{Estimating $E$}
Call Ravikumar's method $R: Z \rightarrow |V| \times |V|$, such that $R(\hat{Z}) = \hat{E}^{(0)}$ gives the estimated edge set. This will be our initial edge set estimate, and we will choose $\hat{E}^{(k)}$ sequentially that is no worse on average than $\hat{E}^{(k-1)}$ for $k \geq 1$.

\section{EM Method}
Given misclassified nodes $M_g$, if we choose to re-estimate the neighbors of these nodes, in the Ravikumar method we use all nodes as neighbor candidates for any given node.  In order to do an EM, we need to reduce the candidate set in order to keep the method computationally tractable.  

Note that if $(A, B) \in E$ and $(B, C) \in E$ but $(A, C) \not\in E$, then if the nodes were not misclassified then $A|B \perp\!\!\!\!\perp C|B$, but if $B$ is misclassified, then we have

\begin{align*}
P(A=1, C=1|\hat{B}=1) &= P(B = \hat{B})P(A=1, C=1|B=1) + P(B\neq \hat{B})P(A=1, C=1|B = 0) \\
&= (1-\tilde\gamma_B)P(A=1|B=1)P(C=1|B=1) + \tilde\gamma_BP(A =1|B=0)P(C=1|B=0) \\
&\neq P(A=1|\hat{B}=1)P(C=1|\hat{B}=1)
\end{align*}

Thus these nodes are no longer independent as long as $\theta_{AB} \neq \theta_{BC}$.  On the other hand, if a node's shortest path to a misclassified node in the true network is greater than or equal to 2, then that node's neighbors will still be chosen independently from the misclassification.  

Taking this into account, we can say that nodes within distance 2 from a misclassified node could potentially have been neighbors with the misclassified node, but nodes further than distance 2 from a misclassified node are more likely to be not affected by this misclassification.

Question: does lack of independence imply higher chance of nodes being selected as neighbors?  Note that if $\theta_{AB}, \theta_{BC} > 0$, then 


\subsection{E Step}

\subsection{M Step}














\end{document}
